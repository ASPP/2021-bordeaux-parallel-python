{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ccae941",
   "metadata": {},
   "source": [
    "# (truly) Parallel Python\n",
    "Unfortunately, not all problems are of the \"embarrassingly-parallel\" type. For example, simulations of dynamical systems (nervous/weather/quantum systems) can easily become too computationally heavy for single machines, both in terms of memory and compute. One solution is to *distribute* such simulations across multiple machines. In particular this implies that we are using multiple processes which all work on a part of the simulation and hence need to communicate. The de-facto standard protocol for inter-process communication (in academia) is the Message Passing Interface (MPI). This protocol defines a standard way of processes to send data to/receive data from each other.\n",
    "\n",
    "Compared to the application we've considered so far, using MPI *effectively* requires significant cognitive and development overhead, so you should very carefully evaluate whether you need to get your hands this dirty before reimplementing your simulation (or do it for fun as a challenge while your supervisor is on holidays ;) ). In the following we will focus on the `mpi4py` package."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a21791d",
   "metadata": {},
   "source": [
    "## Starting multiple processes: who am I?\n",
    "Using MPI requires you to change the way you start your Python program. First, we can not (easily) run it from a jupyter notebook. Second, instead of calling it like `python <script>` from the commandline, we need to run it via the `mpirun` executable. At this point, we also specify how many processes we'd like to start via `-np <number of processes>`. How many processes we use should reflect both the available hardware as well as our choices of how to distribute work.\n",
    "\n",
    "In MPI each process is assigned a rank. This helps you to organize work (\"rank X does Y\") and communication (\"rank X sends Z to rank Y\"). We first implement (one of) the simplest possible MPI program(s): report your rank and exit.\n",
    "\n",
    "WARNING: Here we use MPI for a single machine (which is not the optimal use case; typically you want to use it for distributing work across *many* machines which are connected via network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8417734e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load mpi_hello_world.py\n",
    "from mpi4py import MPI\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "print(f\"Hello world from rank {rank}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9baa7f99",
   "metadata": {},
   "source": [
    "## Performing different work on different processes\n",
    "As mentioned, using the rank, we can let different processes do different work, for example, generating random numbers in different ranges:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5934d7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load mpi_random_numbers.py\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "x = np.random.normal(loc=rank, size=10_000)\n",
    "\n",
    "print(f\"Hello world from rank {rank}. My mean is {np.mean(x)}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76f4f7e",
   "metadata": {},
   "source": [
    "So far, the ranks are acting independently. The distinguishing feature of MPI is that is allows communication between processes. In the following we consider *one particular view* of parallelization across processes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8373c8e6",
   "metadata": {},
   "source": [
    "## Distributed simulation (don't do it like this, though)\n",
    "Now, let's consider a (super simplified) dynamical systems simulation. We have particles moving in a one-dimensional \"box\" between 0 and 1. Assuming lots and lots of particles, we may want to split the work of propagating the particles, i.e., computing their new position, across different processes. Here we decide that each process should propagate the particles within a certain \"volume\" of the box. For example, using two processes, one of them propagates all particles between 0 and 0.5, the other all particles between 0.5 and 1. Of course particles can cross the boundary from below 0.5 to above 0.5 and we hence need to communicate positions between the processes. Here, we go for a simple implementation: after each propagation step, information about the new positions is shared across all ranks. Each rank afterwards determines which particles it should propagate in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453f5fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load mpi_particles.py\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from lib_particles import Particle, propagate_particle\n",
    "\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "n_ranks = comm.Get_size()\n",
    "\n",
    "rng = np.random.RandomState(1235 + rank)\n",
    "\n",
    "n_particles = 12\n",
    "n_steps = 5\n",
    "\n",
    "# determine work distribution between ranks\n",
    "x_min = rank / n_ranks\n",
    "x_max = x_min + 1 / n_ranks\n",
    "\n",
    "# initialize local particles assuming homogeneity\n",
    "n_particles_per_process = int(np.round(n_particles / n_ranks))\n",
    "local_particles = [\n",
    "    Particle(n_particles_per_process * rank + idx, rng.uniform(x_min, x_max))\n",
    "    for idx in range(n_particles_per_process)\n",
    "]\n",
    "print(f\"initial configuration ({rank=}):\",\n",
    "      list(sorted(local_particles, key=lambda p: p.idx)))\n",
    "\n",
    "# simulate for a couple of time steps\n",
    "for step in range(n_steps):\n",
    "\n",
    "    # propagate particle positions\n",
    "    for particle in local_particles:\n",
    "        propagate_particle(particle)  # TODO check implemention propagate_particle -> \"update_particle_position\"\n",
    "        # or particle.x = get_new_particle_position(particle.x)\n",
    "\n",
    "    # communicate particle positions\n",
    "    local_particles_new = []\n",
    "    for root in range(n_ranks):\n",
    "        recv_buffer = comm.bcast(local_particles, root=root)  # TODO mention synchronization/blocking\n",
    "        # WARNING: would not work! local_particles = [particle for particle in recv_buffer if ...]\n",
    "        for particle_new in recv_buffer:\n",
    "            if x_min <= particle_new.x < x_max:\n",
    "                local_particles_new.append(particle_new)\n",
    "    local_particles = local_particles_new\n",
    "\n",
    "    print()\n",
    "    print(f\"{step=} ({rank=}):\",\n",
    "          list(sorted(local_particles, key=lambda p: p.idx)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7949972",
   "metadata": {},
   "source": [
    "Outlook\n",
    "- optimizations/different communication patterns: e.g., send data only to the processes that need it, interleaving communication and computation\n",
    "- (most) common issue: deadlocks, i.e., some processes are waiting for messages that they never receive :O\n",
    "- as soon as you start using bigger systems, you will have to go through a scheduler to run your scripts, e.g., SLURM, LoadLeveler, etc.; this means writing a (bash) script which tells the scheduler what kind of resources you need and how to run your scripts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
