{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ccae941",
   "metadata": {},
   "source": [
    "# (truly) Parallel Python\n",
    "Unfortunately, not all problems are of the \"embarrassingly-parallel\" type. For example, simulations of dynamical systems (nervous/weather/quantum systems) can easily become too computationally heavy for single machines, both in terms of memory and compute. One solution is to *distribute* such simulations across multiple machines. In particular this implies that we are using multiple processes which all work on a part of the simulation and hence need to communicate. The de-facto standard protocol for inter-process communcation (in academia) is the Message Passing Interface (MPI). This protocol defines a standard way of processes to send data to/receive data from each other. Compared to the application we've considered so far, using MPI *effectively* requires significant cognitive and development overhead, so you should very carefully evaluate whether you need to get your hands this dirty before reimplementing your simulation (or do it for fun as a challenge while your supervisor is on holidays ;) ). In the following we will focus on the `mpi4py` package."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2fe1b2",
   "metadata": {},
   "source": [
    "## Starting multiple processes: who am I?\n",
    "Using MPI requires you to change the way you start your Python program. First, we can not (easily) run it from a jupyter notebook. Second, instead of calling it like `python <script>` from the commandline, we need to run it via the `mpirun` executable. At this point, we also specify how many processes we'd like to start via `-np <number of processes>`. In MPI each process is assigned a rank. This helps you to organize work (\"rank X does Y\") and communication (\"rank X sends Z to rank Y\"). We first implement (one of) the simplest possible MPI program(s): report your rank and exit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a8f9330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world from rank 0\n"
     ]
    }
   ],
   "source": [
    "from mpi4py import MPI\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "print(f'hello world from rank {rank}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b35ed8",
   "metadata": {},
   "source": [
    "As mentioned, using the rank, we can let different processes do different work, for example, generating random numbers in different ranges:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "960ec904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world from rank 0. my mean is 0.7738655887422622.\n"
     ]
    }
   ],
   "source": [
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "x = np.random.normal(loc=rank)\n",
    "\n",
    "print(f'hello world from rank {rank}. my mean is {np.mean(x)}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864b9bac",
   "metadata": {},
   "source": [
    "- simple example of mpi program (report rank and exit)\n",
    "- explain different way of calling it (with mpirun executable)\n",
    "- discuss issue of unordered execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c79969",
   "metadata": {},
   "source": [
    "Now, let's consider a (super simplified) dynamical systems simulation. We have particles moving in a one-dimensional \"box\" between 0 and 1. Assuming lots and lots of particles, we may want to split the work of propagating the particles, i.e., computing their new position, across different processes. Here we decide that each process should propagate the particles within a certain \"volume\" of the box. For example, using two processes, one of them propagates all particles between 0 and 0.5, the other all particles between 0.5 and 1. Of course particles can cross the boundary from below 0.5 to above 0.5 and we hence need to communicate positions between the processes. Here, we go for a simple implementation: after each propagation step, information about the new positions is shared across all ranks. Each rank afterwards determines which particles it should propagate in the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e5474d",
   "metadata": {},
   "source": [
    "- send/recv example\n",
    "- discuss issue of telling each rank how to behave\n",
    "- discuss issue of blocking\n",
    "- discuss issue of what can be send between ranks (pickleable?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fad3d3a",
   "metadata": {},
   "source": [
    "# TODO\n",
    "- caveat `send` vs `Send`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae6b609",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
