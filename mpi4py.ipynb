{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ccae941",
   "metadata": {},
   "source": [
    "# (Distributed) Parallel Python\n",
    "Unfortunately, not all problems are of the \"embarrassingly parallel\" type. For example, simulations of dynamical systems (nervous/weather/quantum systems) can easily become too computationally heavy for single machines, both in terms of memory and compute. One solution is to *distribute* such simulations across multiple machines. In particular this implies that we are using multiple processes which all work on a part of the simulation and hence need to communicate. The de-facto standard protocol for inter-process communication (in academia) is the Message Passing Interface (MPI).\n",
    "\n",
    "## What is MPI?\n",
    "MPI is a standard for passing data (\"messages\") between processes. In the case of `mpi4py` the data typically consists of pickled Python objects. As a user, you don't need to worry how the data gets from one process to the other (possibly via a local network), the MPI implementation takes care of that for you.\n",
    "\n",
    "#### WARNING\n",
    "Compared to the solutions we've considered so far, using MPI *effectively* requires significant cognitive and development overhead, so you should very carefully evaluate whether you need to get your hands this dirty before reimplementing your simulation (or do it for fun as a challenge while your supervisor is on holidays ;) ). In the following we will focus on the `mpi4py` package.\n",
    "\n",
    "#### How to install MPI?\n",
    "Briefly: Don't. Typically you'll want to use MPI on some cluster/supercomputer on which the system administrator has taken care of installing and configuring an MPI library. You can also install it on your machine, for Linux typically via the package manager."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a21791d",
   "metadata": {},
   "source": [
    "## Starting multiple processes: who am I?\n",
    "Using MPI requires you to change the way you start your Python program. First, we can not (easily) run it from a jupyter notebook. Second, instead of calling it like `python <script>` from the commandline, we need to run it via the `mpirun` executable. At this point, we also specify how many processes we'd like to start via `-np <number of processes>`. How many processes we use should reflect both the available hardware as well as our choices of how to distribute work.\n",
    "\n",
    "In MPI each process is assigned a index (\"rank\" in MPI lingo). This helps you to organize work (\"rank X does Y\") and communication (\"rank X sends Z to rank Y\"). We first implement (one of) the simplest possible MPI program(s): report your rank and exit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8417734e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load mpi_hello_world.py\n",
    "from mpi4py import MPI\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "process = comm.Get_rank()\n",
    "\n",
    "print(f\"Hello world from {process = }\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9baa7f99",
   "metadata": {},
   "source": [
    "## Performing different work on different processes\n",
    "As mentioned, using the rank, we can let different processes do different work, for example, for a dynamical systems simulation, we can use it to determine initial positions of particles. For reproducibility, we would like to fix the seed of the random number generator. At the same time we want to make sure that different processes are using different number streams. A simple way to achieve this is to use some fixed seed in combination with the rank.\n",
    "\n",
    "Let's see how we would generate initial conditions for a dynamical systems simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5934d7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load mpi_random_numbers.py\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "comm = MPI.COMM_WORLD  # get MPI communicator\n",
    "process = comm.Get_rank()  # get process index\n",
    "n_processes = comm.Get_size()  # get total number of processes\n",
    "\n",
    "# initialize a random number generator for each process\n",
    "rng = np.random.RandomState(1235 + process)\n",
    "\n",
    "# define the number of particles per processes and time steps to simulate\n",
    "n_particles_per_process = 3\n",
    "n_particles = n_particles_per_process * n_processes\n",
    "n_time_steps = 5\n",
    "\n",
    "# determine work distribution between ranks\n",
    "x_min = process / n_processes\n",
    "x_max = (process + 1) / n_processes\n",
    "\n",
    "# generate initial positions\n",
    "x = np.random.uniform(x_min, x_max, size=n_particles_per_process)\n",
    "\n",
    "print(f\"{process=}: particle positions {x}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76f4f7e",
   "metadata": {},
   "source": [
    "So far, the ranks are acting independently. The distinguishing feature of MPI is that is allows communication between processes. In the following we consider *one particular view* of parallelization across processes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8373c8e6",
   "metadata": {},
   "source": [
    "## Distributed simulation\n",
    "Now, let's consider a (super simplified) dynamical systems simulation. We have particles moving in a one-dimensional \"box\" between 0 and 1. Assuming lots and lots of particles, we may want to split the work of propagating the particles, i.e., computing their new position, across different processes. Here we decide that each process should propagate the particles within a certain \"volume\" of the box. For example, using two processes, one of them propagates all particles between 0 and 0.5, the other all particles between 0.5 and 1. Of course particles can cross the boundary from below 0.5 to above 0.5 and we hence need to communicate positions between the processes. Here, we go for a simple implementation: after each propagation step, information about the new positions is shared across all ranks. Each rank afterwards determines which particles it should propagate in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453f5fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load mpi_particles.py\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from lib_particles import Particle, compute_new_particle_position\n",
    "\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "process = comm.Get_rank()\n",
    "n_processes = comm.Get_size()\n",
    "\n",
    "# initialize a random number generator for each process\n",
    "rng = np.random.RandomState(1235 + process)\n",
    "\n",
    "# define the number of particles per processes and time steps to simulate\n",
    "n_particles_per_process = 3\n",
    "n_particles = n_particles_per_process * n_processes\n",
    "n_time_steps = 5\n",
    "\n",
    "# determine work distribution between ranks\n",
    "x_min = process / n_processes\n",
    "x_max = (process + 1) / n_processes\n",
    "\n",
    "# initialize local particles assuming homogeneity\n",
    "local_particles = [\n",
    "    Particle(n_particles_per_process * process + idx, rng.uniform(x_min, x_max))\n",
    "    for idx in range(n_particles_per_process)\n",
    "]\n",
    "print(\n",
    "    f\"initial configuration ({process=}):\",\n",
    "    list(sorted(local_particles, key=lambda p: p.idx)),\n",
    ")\n",
    "# exit()\n",
    "\n",
    "# simulate dynamics for a couple of time steps\n",
    "for time_step in range(n_time_steps):\n",
    "\n",
    "    # propagate particle positions\n",
    "    for particle in local_particles:\n",
    "        particle.x = compute_new_particle_position(particle.x)\n",
    "\n",
    "    # communicate particle positions\n",
    "    received_particles = []\n",
    "    for source_process in range(n_processes):\n",
    "        # we communicate the list `local_particles` from rank `root` to all\n",
    "        # other ranks; all ranks receive the same list from rank `root`, which\n",
    "        # is stored in `recv_buffer`; communication is *blocking*, i.e., all\n",
    "        # ranks wait here until the have received the data\n",
    "        received_particles += comm.bcast(local_particles, root=source_process)\n",
    "\n",
    "    assert len(received_particles) == n_particles\n",
    "\n",
    "    # after the communication is done, we figure out which particles this\n",
    "    # rank now needs to keep track off\n",
    "    local_particles = []\n",
    "    for particle_new in received_particles:\n",
    "        if x_min <= particle_new.x < x_max:\n",
    "            local_particles.append(particle_new)\n",
    "\n",
    "    print()\n",
    "    print(\n",
    "        f\"{time_step=} ({process=}):\",\n",
    "        list(sorted(local_particles, key=lambda p: p.idx)),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39756016",
   "metadata": {},
   "source": [
    "## Outlook\n",
    "- optimizations/different communication patterns: e.g., send data only to the processes that need it, interleaving communication and computation\n",
    "- (most) common issue: deadlocks, i.e., some processes are waiting for messages that they never receive :O\n",
    "- as soon as you start using bigger systems, you will have to go through a scheduler to run your scripts, e.g., SLURM, LoadLeveler, etc.; this means writing a (bash) script which tells the scheduler what kind of resources you need and how to run your scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bc5526",
   "metadata": {},
   "source": [
    "### Resources\n",
    "- https://mpitutorial.com/\n",
    "- https://www.open-mpi.org/doc/\n",
    "- https://www.open-mpi.org/doc/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4ab3ae",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
