{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embarassingly-parallel Python\n",
    "In the following we focus on \"embarrassingly parallel\" problems, i.e., problems which can be easily parallelized across threads or processes as they do not require interaction while running. This may include reading a collection of large files, querying databases, or multiple trials of neural networks, molecular dynamics simulations or sampling methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Threading in Python\n",
    "We first work with the `ThreadPool` available from the multiprocessing module. We assume CPython in which the GIL prevent several threads from executing in parallel. However, for some use case, in particular those which are **I/O bound**, threading can be very useful. Consider for example obtaining data from some database: you would like to query a couple of measurements, and completing each of these queries may take some processing time on the server. Here we mimick this server-side processing time by merely sleeping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing.pool import ThreadPool\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_database(x):\n",
    "    \"\"\"Query your database to retrieve awesome measurements.\"\"\"\n",
    "    time.sleep(x)  # mimicks (input-dependent) server-side processing\n",
    "    y = x ** 2\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [1, 8, 1.5, 2]  # some dummy queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we use the builtin `map` function to perform the database query for each item in l:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 64, 2.25, 4]\n",
      "CPU times: user 2.01 ms, sys: 487 µs, total: 2.5 ms\n",
      "Wall time: 12.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "result = list(map(query_database, l))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations?\n",
    "- total duration is the sum of the duration of each query -> queries processed in serial, one after the other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we use use `ThreadPool` do perform these queries using two threads (here the `processes` argument actually refers to the number of threads):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 64, 2.25, 4]\n",
      "CPU times: user 8.32 ms, sys: 832 µs, total: 9.15 ms\n",
      "Wall time: 8.03 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with ThreadPool(processes=2) as pool:  # context manager providing a `ThreadPool` instance\n",
    "    result = pool.map(query_database, l)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations?\n",
    "- results are identical to serial processing of queries; good!\n",
    "- total duration is reduced: work (here: waiting for results) is distributed across threads\n",
    "- allocation: queries are performed in order; thread 0 works on query 0, thread 1 on query 1, thread 0 on the rest while thread 1 is busy with query 1\n",
    "- careful: load is not automatically balanced (`ThreadPool` can not know how long each query takes); in our example if long query is the last, total duration increases "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [1, 1.5, 2, 8]  # some dummy queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2.25, 4, 64]\n",
      "CPU times: user 5.92 ms, sys: 1.41 ms, total: 7.33 ms\n",
      "Wall time: 9.52 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with ThreadPool(processes=2) as pool:\n",
    "    result = pool.map(query_database, l)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's consider a compute-intense number-crunching task, for example simulating multiple trials of our fancy neural network model. Here we mimick the simulation by merely counting down from a large number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crunch_numbers(x):\n",
    "    \"\"\"Run a simulation of your favourite neural network model.\"\"\"\n",
    "    n = x * 1e7  # mimick compute-intense simulation\n",
    "    while n > 0:\n",
    "        n -= 1\n",
    "    y = x ** 2\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [1, 8, 1.5, 2]  # some dummy simulations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, first, we use the builtin `map` function to perform the number crunching for each item in l:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 64, 2.25, 4]\n",
      "CPU times: user 9.9 s, sys: 2.16 ms, total: 9.9 s\n",
      "Wall time: 9.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "result = list(map(crunch_numbers, l))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we use again use `ThreadPool` do perform these simulations in parallel using two threads:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 64, 2.25, 4]\n",
      "CPU times: user 10.2 s, sys: 24.2 ms, total: 10.3 s\n",
      "Wall time: 10.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with ThreadPool(processes=2) as pool:\n",
    "    result = pool.map(crunch_numbers, l)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations?\n",
    "- runtime (almost) identical to serial execution: GIL prevents simultaneous number crunching. :'("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiprocessing in Python\n",
    "We now introduce the `ProcessPool`. In contrast to the `ThreadPool` this context manager distributes work across multiple processes running separate instances of the Python interpreter. This allows you to circumvent the limitations of the GIL and achieve truly parallel code execution. Unfortunately, it introduces some downsides, such as some overhead for launching processes and increased memory consumption (depending on implementation and use case). However, for use cases which are **compute bound**, it is an excellent, simple-to-use option. As already introduced above, these use cases may include numerical simulations, sampling methods etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing.pool import Pool as ProcessPool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 64, 2.25, 4]\n",
      "CPU times: user 10.3 ms, sys: 8.06 ms, total: 18.4 ms\n",
      "Wall time: 6.74 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with ProcessPool(processes=2) as pool:  # context manager providing a `Pool` instance\n",
    "    result = pool.map(crunch_numbers, l)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations?\n",
    "- result are identical to serial and threaded execution; good!\n",
    "- runtime is reduced compare to both serial and threaded execution\n",
    "- as before, work is distributed across processes one by one, no automatic load balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [1, 1.5, 2, 8]  # some dummy simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2.25, 4, 64]\n",
      "CPU times: user 12.9 ms, sys: 3.98 ms, total: 16.8 ms\n",
      "Wall time: 7.82 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with ProcessPool(processes=2) as pool:\n",
    "    result = pool.map(crunch_numbers, l)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TBD\n",
    "- race conditions for threading\n",
    "- memory sharing for processes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
