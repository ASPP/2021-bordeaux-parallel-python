{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serial Python\n",
    "\n",
    "Before we get into parallel processing, let's first consider how the type of problems we'll consider are typical solved in a serial context. We'll focus on problems which always have the following form: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_science(x):\n",
    "    \"\"\"For example:\n",
    "    - training a neural network (hyperparameter tuning!)\n",
    "    - getting results from a database\n",
    "    - scraping some websites\n",
    "    - read files\n",
    "    - sampling monte-carlo style\n",
    "    \"\"\"\n",
    "    return x ** 2  # we don't really do anything ;)\n",
    "\n",
    "results = []\n",
    "input_data = range(10)\n",
    "for x in input_data:\n",
    "    results.append(do_science(x))\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This typical structure (or \"smell\" ;) ) is pretty common and most likely all of you have something similar somewhere in your code. It's an excellent opportunity for leveraging parallelism to speed things up. However, first we'll rewrite this code using the builtin `map` function, it makes the code more compact and it will be easier to make this run in parallel. `map` takes a function and an iterable and applies the function to each iterable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = map(do_science, input_data)\n",
    "print(list(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(watch out: `map` returns an iterator (->advanced python), you need to convert it to a list explicitly)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embarassingly-parallel Python\n",
    "This type of problem is referred to as \"embarrassingly parallel\" problems. This indicates that they can be easily parallelized across threads or processes as they do not require interaction while running (they can also be run in serial!). For these types of problems, we can use the builtin `multiprocessing` module. It supports parallel versions of `map` which can be run either in parallel threads or parallel processes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel threads\n",
    "We first work with the `ThreadPool` available from the `multiprocessing.pool` module. We assume CPython in which the GIL prevent several threads from executing in parallel. However, for some use case, in particular those which are **I/O bound**, threading can be very useful. Consider for example obtaining data from some database: you would like to query a couple of measurements, and completing each of these queries may take some processing time on the server. Here we mimick this server-side processing time by merely sleeping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing.pool import ThreadPool\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_database(x):\n",
    "    \"\"\"Query your database to retrieve awesome measurements.\"\"\"\n",
    "    print(f'querying data {x} start')\n",
    "    time.sleep(x)  # mimicks (input-dependent) server-side processing\n",
    "    y = x ** 2\n",
    "    print(f'querying data {x} end')\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [1, 8, 1.5, 2]  # some dummy queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we use the builtin `map` function to perform the database query for each item in l:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "result = list(map(query_database, l))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations?\n",
    "- queries processed in serial, one after the other\n",
    "- total duration is the sum of the duration of each query \n",
    "- sleep (like I/O bound tasks) does not cause high CPU load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we use use `ThreadPool` do perform these queries using two threads (here the `processes` argument actually refers to the number of threads):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with ThreadPool(processes=2) as pool:  # context manager providing a `ThreadPool` instance\n",
    "    result = pool.map(query_database, l)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations?\n",
    "- results are identical to serial processing of queries; good!\n",
    "- total duration is reduced: work (here: waiting for results) is distributed across threads\n",
    "- allocation: queries are performed in order; thread 0 works on query 0, thread 1 on query 1, thread 0 on the rest while thread 1 is busy with query 1\n",
    "- caveat: printing might get messy when printing from many threads\n",
    "- caveat: optimal number of threads may be difficult to determine (more threads also means for switching!)\n",
    "- caveat: load is not automatically balanced (`ThreadPool` can not know how long each query takes); in our example if long query is the last, total duration increases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [1, 1.5, 2, 8]  # some dummy queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with ThreadPool(processes=2) as pool:\n",
    "    result = pool.map(query_database, l)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "A once-in-a-lifetime opportunity presents itself: you have access to a database holding secret information about ASPP. You can sent queries to it using the `query_secret_aspp_database` function, which expects a single (arbitrary) argument. First, implement a serial version, processing several (4+) queries. Then use the `ThreadPool` to parallelize your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from secret_aspp_database import query_secret_aspp_database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# TODO fill me with the serial version\n",
    "queries = ['42', 3.1415, 8162371823.123, 'pelita', 'ajwdiauwdiuahwiduhawdiuhawd']\n",
    "list(map(query_secret_aspp_database, queries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with ThreadPool(processes=2) as pool:\n",
    "    result = pool.map(query_secret_aspp_database, queries)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thread-parallel(?) number crunching\n",
    "Now let's consider a compute-intense number-crunching task, for example tuning hyperparameters our fancy neural network model to squeeze out the additional 0.0002% increase in accuracy. Here we mimick the training by merely counting down from a large number (let's avoid cognitive overhead)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_neural_network(x):\n",
    "    \"\"\"Train your favourite neural network model.\"\"\"\n",
    "    print(f'training with {x} start')\n",
    "    n = x * 2e7  # mimick compute-intense training\n",
    "    while n > 0:\n",
    "        n -= 1\n",
    "    y = x ** 2\n",
    "    print(f'training with {x} end')\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [1, 8, 1.5, 2]  # some dummy simulations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, first, we use the builtin `map` function to perform the number crunching serially for each item in l:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "result = list(map(train_neural_network, l))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "- number crunching causes high CPU load (surprise! ;) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we use again use `ThreadPool` do parallelize this work using two threads:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with ThreadPool(processes=2) as pool:\n",
    "    result = pool.map(train_neural_network, l)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations?\n",
    "- runtime (almost) identical to serial execution: GIL prevents simultaneous number crunching. :'("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel processes\n",
    "For such tasks, we use the `ProcessPool`. In contrast to the `ThreadPool` this distributes work across multiple processes running separate instances of the Python interpreter. This allows you to circumvent the limitations of the GIL and achieve truly parallel code execution. For use cases which are **compute bound**, it is an excellent, simple-to-use option. As already introduced above, these use cases may include numerical simulations, sampling methods etc. Unfortunately, using multiple processes introduces some downsides, such as some overhead (time & memory) for launching processes and increased memory consumption (e.g., duplication of data; warning: depends on implementation and use case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing.pool import Pool as ProcessPool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with ProcessPool(processes=2) as pool:  # context manager providing a `Pool` instance\n",
    "    result = pool.map(train_neural_network, l)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations?\n",
    "- result are identical to serial and threaded execution; good!\n",
    "- runtime is reduced compare to both serial and threaded execution\n",
    "- increased CPU load on multiple cores\n",
    "- caveat: as before, no automatic load balancing, tasks are executed in order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel speedup\n",
    "So how much faster does my code become when I'm increasing the number of processes? Here we investiate the relative speedup (T_parallel / T_serial) for an increasing number of processes. We use the same compute-bound function as before, but remove some of the annoying output and make it a bit shorter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train_neural_network(x):\n",
    "    \"\"\"Train your favourite neural network model.\"\"\"\n",
    "    n = x * 1e6  # mimick compute-intense training\n",
    "    while n > 0:\n",
    "        n -= 1\n",
    "    y = x ** 2\n",
    "    return y\n",
    "\n",
    "\n",
    "l = [2] * 16  # some dummy simulations of equal duration\n",
    "times = []\n",
    "for n_processes in range(1, 9):\n",
    "    t0 = time.time()\n",
    "    with ProcessPool(processes=n_processes) as pool:\n",
    "        result = pool.map(train_neural_network, l)\n",
    "    times.append(time.time() - t0)\n",
    "\n",
    "times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "times = np.array(times)\n",
    "plt.plot(range(1, 9), times[0] / times, marker='o', label='measured')\n",
    "plt.plot(range(1, 9), range(1, 9), color='k', label='ideal')\n",
    "plt.xlabel(r'$n$ processes')\n",
    "plt.ylabel('relative speedup')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations\n",
    "- perfect speedup with 2-3 processes, good speedup until ~4-5 processes with decreasing benefits\n",
    "- no (significant) benefits for more processes\n",
    "- rule of thumb: benefits up to number of cores (OS also needs some compute: context switching; also hyperthreading does not seem to work well in my experience)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "Implement your own compute-bound function. Compare it's runtime for different numbers of processes, similar as in our example above. In particular investigate the case of \"fast\" compute-bound functions. Discuss your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crunch_numbers(x):\n",
    "    pass  # TODO replace me with some number crunching\n",
    "\n",
    "l = None  # TODO replace with meaningful input to your function\n",
    "times = []\n",
    "for n_processes in range(1, 9):\n",
    "    t0 = time.time()\n",
    "    with ProcessPool(processes=n_processes) as pool:\n",
    "        result = pool.map(crunch_numbers, l)\n",
    "    times.append(time.time() - t0)\n",
    "\n",
    "times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "https://www.youtube.com/watch?v=AG1soUh4-nU\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
