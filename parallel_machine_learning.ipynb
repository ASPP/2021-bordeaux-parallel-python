{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81d17091",
   "metadata": {},
   "source": [
    "# Parallelizing a simple grid search\n",
    "\n",
    "When fitting a neural net, it is often unclear what values of the hyperparameters will produce the model with the lowest error. A common method to yield more insight is to perform a hyperparameter grid search, where we fit the model for different sets of hyperparameters on a specified grid.\n",
    "\n",
    "This notebook will go over a simple grid search for a Support Vector Classifier and how we can parallelize it using `scikit-learn` and `dask`. The majority of its content is taken from [this dask tutorial](https://github.com/dask/dask-tutorial/blob/main/08_machine_learning.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372a9500",
   "metadata": {},
   "source": [
    "## 0. Create the dataset\n",
    "\n",
    "Our hypothetical problem has 4 features, and we have 10,000 samples. Each sample of 4 features is classified to either 0 or 1. We use `scikit-learn`'s ability to make random datasets for testing. We can look at a few samples of inputs and outputs and plot them to get an idea of what our dataset looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a571806f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(n_samples=13_000, n_features=4, random_state=0)\n",
    "X[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6e6587",
   "metadata": {},
   "outputs": [],
   "source": [
    "y[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486188b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nplot = 1000  # no. of points to plot\n",
    "ix, iy = 0, 1  # which features to plot on x and y\n",
    "\n",
    "fig, ax = plt.subplots(1, clear=True, figsize=(6, 5))\n",
    "ax.grid();\n",
    "ax.scatter(X[:nplot, ix], X[:nplot, iy], c=y[:nplot], zorder=4, alpha=0.8)\n",
    "ax.set_xlabel(f'Feature {ix}')\n",
    "ax.set_ylabel(f'Feature {iy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e88810c",
   "metadata": {},
   "source": [
    "## 1. Fit a Support Vector Classifier\n",
    "\n",
    "Let's start by fitting a single SVC model using a hard-coded set of hyperparameters. Note how long it takes to fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1909e6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "estimator = SVC(C=1.0, kernel='poly', gamma='auto', random_state=0, probability=True)\n",
    "\n",
    "estimator.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99af2a35",
   "metadata": {},
   "source": [
    "We can quantify how good the model works using the `estimator.score` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6001f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25422b1d",
   "metadata": {},
   "source": [
    "## 2. Hyperparameter optimization (no parallelization)\n",
    "\n",
    "There are a few ways to learn the best hyperparameters while training. One is GridSearchCV. As the name implies, this does a brute-force search over a grid of hyperparameter combinations.\n",
    "\n",
    "We start by defining our grid of hyperparameters that we want to search. We'll also redefine our estimator that we'll use in the grid search. Finally, we will be using something called 2-fold validation, which means that we will run every computation twice for a more robust solution. The k-fold cross-validation is specified using the `cv` keyword argument in `GridSearchCV`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d506f815",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = SVC(gamma='auto', random_state=0, probability=True)\n",
    "param_grid = {\n",
    "    'C': [0.001, 10.0],\n",
    "    'kernel': ['rbf', 'poly'],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd2e875",
   "metadata": {},
   "source": [
    "**Exercise together:** Considering that our grid has (1) two values for `C` and (2) two values for `kernel` and (3) 2-fold cross validation, about how long do we think it would take to search the grid without parallelization? Use the fitting time from the previous section.\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37cf39f",
   "metadata": {},
   "source": [
    "Let's go ahead and test that. Does it match?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c2f808",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "grid_search = GridSearchCV(estimator, param_grid, verbose=2, cv=2)\n",
    "grid_search.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3813368c",
   "metadata": {},
   "source": [
    "Here are the values and score for the best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba88fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_best(grid_search):\n",
    "    \"\"\"Print the values and score for the grid_search result\"\"\"\n",
    "    best_svc = grid_search.best_estimator_\n",
    "    print(f'The best model has C = {best_svc.C} and kernel = {best_svc.kernel}')\n",
    "    print(f'  Model score: {grid_search.best_score_:.4f}')\n",
    "\n",
    "print_best(grid_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1699d0",
   "metadata": {},
   "source": [
    "## 3. Parallelization with scikit-learn\n",
    "\n",
    "**Exercise together:** Considering the number of CPUs on your laptop, how fast do you expect the code to run if it is parallelized using multiple processes?\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38c86a4",
   "metadata": {},
   "source": [
    "The `GridSearchCV` in `scikit-learn` has been written to allow basic parallelization. In particular, you can use the `n_jobs` keyword, which will result in multiple processes being launched.  You can specify an integer number of processes, or `-1` means all available CPUs.\n",
    "\n",
    "Note! On Windows, calling multiple processes will mess up printing to the console. Expect no verbose output.\n",
    "\n",
    "**Exercise for the reader:** How does the wall time compare? Did it match your prediction? Why/why not?\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ff103e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "grid_search = GridSearchCV(estimator, param_grid, verbose=2, cv=2, n_jobs=-1)\n",
    "grid_search.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0ad85d",
   "metadata": {},
   "source": [
    "Verify that we got the same solution for the grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e5c4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_best(grid_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcbc854",
   "metadata": {},
   "source": [
    "## 4. Parallelization with dask\n",
    "\n",
    "**Exercise together:** The `scikit-learn` `n_jobs` option is good for this problem, but can you think of a situation (or two) where it would be insufficient?\n",
    "\n",
    "**Answer:** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81b3036",
   "metadata": {},
   "source": [
    "Dask can also be used to parallelize, and it has the ability to be run across multiple machines (e.g., on a cluster). For dask to run, we first define something called a \"client\", which is used to communicate with the processors. We will set up a simple client on the desktop for this exercise, but clients can also be launched on clusters. Because that is very cluster-specific, we won't discuss that here.\n",
    "\n",
    "**Note!** If you have larger-than-memory datasets, you might want to check out some of the estimators in `dask_ml`. You can see an\n",
    "\n",
    "Be sure to call the client using `c.close()` before closing the notebook or goblins will appear from the 4th hellish dimension. (Not really, just may leave some ports open that we want closed.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7b3778",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.distributed\n",
    "\n",
    "c = dask.distributed.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38892349",
   "metadata": {},
   "source": [
    "One cool thing about dask is that we can open a dashboard showing the status of our parallelization task. Open the following link in a browser.\n",
    "\n",
    "The status window includes the number of bytes per worker (process), what processes are doing which tasks, and how many tasks are happening per process. You can also click the CPU tab on the bottom left to see how well your CPU is being utilized. Again, don't worry about the details on this dashboard. If you want to learn more about dask, there are many tutorials available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bc38ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "c.dashboard_link"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd13ed9",
   "metadata": {},
   "source": [
    "If you run this on a cluster, uncomment lines in the grid below to get a larger search grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03bf135",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'C': [0.001, 10.0],\n",
    "    'kernel': ['rbf', 'poly'],\n",
    "    # Uncomment this for larger Grid searches on a cluster\n",
    "    # 'C': [0.001, 0.1, 1.0, 2.5, 5, 10.0],\n",
    "    # 'kernel': ['rbf', 'poly', 'linear'],\n",
    "    # 'shrinking': [True, False],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator, param_grid, verbose=2, cv=2, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8f18a2",
   "metadata": {},
   "source": [
    "It's finally time to run!\n",
    "\n",
    "We use a context manager `parallel_backend` from the `joblib` package ([docs here](https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend)) to specify that we want out code to run using dask instead of the normal backend. The `scatter` keyword is making sure that each process has a copy of the data before fitting. Don't worry about understanding what `joblib` is doing, it is not super important to the exercise.\n",
    "\n",
    "While the code is running, you can keep an eye on the dask dashboard to see CPU usage, tasks per CPU, and a bunch of other information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b13c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import joblib\n",
    "\n",
    "with joblib.parallel_backend('dask', scatter=[X, y]):\n",
    "    grid_search.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0babaaf6",
   "metadata": {},
   "source": [
    "Verify the correct result and close the client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7a2e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "c.close()\n",
    "print_best(grid_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bd3af6",
   "metadata": {},
   "source": [
    "The simulation time is much slower than with multiprocessing! But this is actually expected: dask creates a lot of overhead and doesn't always evenly assign tasks to workers. \n",
    "\n",
    "One advantage of dask is that can be run on multiple nodes on a cluster using almost exactly the same code as what we have here. Moreover, there are functions in the `dask_ml` library that will work on very large datasets that cannot be loaded into memory. If that is of interest to you, then you can see the last exercise in [this tutorial](https://github.com/dask/dask-tutorial/blob/main/08_machine_learning.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5802de15",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
